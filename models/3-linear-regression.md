---
layout: model
title: Bayesian Linear Regression
---

Suppose we want to predict a one-dimensional scalar output from two-dimensional scalar inputs.

Here's our true generative model, and a dataset generated by sampling from it:

~~~~
var f = function(x, y) {
  return 2*x - 3*y;
};

var data = repeat(20, function(){
  var x = uniform(-5, 5);
  var y = uniform(-5, 5);
  return {
    x: x,
    y: y,
    z: f(x, y)
  };
});

wpEditor.put('data', data);

data   
~~~~

We can use multivariate linear regression to recover this function from data:

~~~~
var data = wpEditor.get('data');

var model = function() {

  var m = sample(DiagCovGaussian({mu: Vector([0, 0]), sigma: Vector([2, 2])}));
  var b = sample(Gaussian({mu: 0, sigma: 2}));
  var sigma = sample(Gamma({shape: 1, scale: 1}));

  var f = function(x) {
    return T.sumreduce(T.mul(m, x)) + b;
  };

  map(
    function(datum) {
      var params = {
        mu: f(Vector([datum.x, datum.y])), 
        sigma: sigma
      };
      factor(Gaussian(params).score(datum.z));
    },
    data);

  return {
    m0: m.data[0],
    m1: m.data[1]
  };
}

var out = Infer({method: 'MCMC', samples: 10000}, model);

viz.auto(out)
~~~~

This works, but MCMC is not an efficient method for solving this problem. HMC is [not yet available](https://github.com/probmods/webppl/issues/286) for multivariate distributions.

We can also approach this problem using mean-field variational inference:

~~~~
///fold:
var showMultivariateDist = function(dist) {
  var f = function() {
    var s = sample(dist);
    return {
      x: s.data[0],
      y: s.data[1]
    }
  };
  var out = Infer({method: 'rejection', samples: 5000}, f);
  viz.auto(out);
};
///

var data = wpEditor.get('data');

var model = function() {
  var mPrior = DiagCovGaussian({
    mu: Vector([0, 0]), 
    sigma: Vector([2, 2])
  });
  var mGuide = DiagCovGaussian({
    mu: param(Vector([0, 0])), 
    sigma: param(Vector([2, 2])) 
  });
  var m = sample(mPrior, {guide: mGuide});
  
  var bPrior = Gaussian({mu: 0, sigma: 2});
  var bGuide = Gaussian({mu: param(0), sigma: param(2)});
  var b = sample(bPrior, {guide: bGuide});

//   var sigmaPrior = Gamma({shape: 1, scale: 1});
//   var sigmaGuide = Gamma({shape: param(1), scale: param(1)});
//   var sigma = sample(sigmaPrior, {guide: sigmaGuide});
  var sigma = 0.5;

  var f = function(x) {
    return T.sumreduce(T.mul(m, x)) + b;
  };
  
  map(
    function(datum) {
      var params = {
        mu: f(Vector([datum.x, datum.y])), 
        sigma: sigma
      };
      var score = Gaussian(params).score(datum.z);
      factor(score);
    },
    data);
  
  return m;
};

var params = Optimize(model, {steps: 2000, method: {adagrad: {stepSize: 0.01}}, estimator: 'ELBO'});
var marginal = SampleGuide(model, {samples: 1000, params: params});

// showMultivariateDist(marginal)

var E_m0 = expectation(marginal, function(x) { return x.data[0]; });
var E_m1 = expectation(marginal, function(x) { return x.data[1]; });

// Expect to see [2, -3]
[E_m0, E_m1]
~~~~