---
layout: model
title: Bayesian Linear Regression
hidden: true
---

Suppose we want to predict a one-dimensional scalar output from two-dimensional scalar inputs.

Here's our true generative model, and a dataset generated by sampling from it:

~~~~
var f = function(x, y) {
  return 2*x - 3*y;
};

var data = repeat(20, function(){
  var x = uniform(-5, 5);
  var y = uniform(-5, 5);
  return {
    x: x,
    y: y,
    z: f(x, y)
  };
});

wpEditor.put('data', data);

data   
~~~~

We can use multivariate linear regression to recover this function from data:

~~~~
var data = wpEditor.get('data');

var model = function() {

  var m = sample(DiagCovGaussian({mu: Vector([0, 0]), sigma: Vector([2, 2])}));
  var b = sample(Gaussian({mu: 0, sigma: 2}));
  var sigma = sample(Gamma({shape: 1, scale: 1}));

  var f = function(x) {
    return T.sumreduce(T.mul(m, x)) + b;
  };

  map(
    function(datum) {
      var params = {
        mu: f(Vector([datum.x, datum.y])), 
        sigma: sigma
      };
      factor(Gaussian(params).score(datum.z));
    },
    data);

  return {
    m0: m.data[0],
    m1: m.data[1]
  };
}

var out = Infer({method: 'MCMC', samples: 10000}, model);

viz.auto(out)
~~~~

This works, but MCMC is not an efficient method for solving this problem. HMC would be better, but is [not yet available](https://github.com/probmods/webppl/issues/286) for multivariate distributions.
